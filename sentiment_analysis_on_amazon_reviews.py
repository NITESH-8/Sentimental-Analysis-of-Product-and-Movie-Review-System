# -*- coding: utf-8 -*-
"""Sentiment Analysis on Amazon Reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nv_D17wK_HyUwTdmzXFJJ4lFDfSjk3Iq
"""

#loading the required libraries

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.python.keras import models, layers, optimizers
import tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
import bz2
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
import re

# %matplotlib inline

from google.colab import files
uploaded = files.upload()



#Creating a function to load the text and labels from train and test set

def get_labels_and_texts(file):
    f=open(file,"r")
    labels = []
    texts = []
    count=0
    while(True):
        line=f.readline()
        if not line:
          break
        labels.append(int(line[9])-1)
        texts.append(line[10:].strip())
    return np.array(labels), texts
train_labels, train_texts = get_labels_and_texts('train_new.ft.txt')
test_labels, test_texts = get_labels_and_texts('test_new.ft.txt')

train_texts[0:5]

print(len(train_texts))
print(len(test_texts))

train_labels=train_labels[0:10000]

train_texts=train_texts[0:10000]

#text pre-processing

import re
NON_ALPHANUM = re.compile(r'[\W]')
NON_ASCII = re.compile(r'[^a-z0-1\s]')
def normalize_texts(texts):
    normalized_texts = []
    for text in texts:
        lower = text.lower()
        no_punctuation = NON_ALPHANUM.sub(r' ', lower)
        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)
        normalized_texts.append(no_non_ascii)
    return normalized_texts
        
train_texts = normalize_texts(train_texts)
test_texts = normalize_texts(test_texts)

train_texts[0]

#countvectorizer

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(binary=True)
cv1=cv.fit(train_texts)
X = cv.transform(train_texts)
X_test = cv.transform(test_texts)

X_test

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


X_train, X_val, y_train, y_val = train_test_split(X, train_labels, train_size = 0.75)

for c in [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1]:
    
    lr = LogisticRegression(C=c)
    lr.fit(X_train, y_train)
    print ("Accuracy for C=%s: %s" % (c, accuracy_score(y_val, lr.predict(X_val))))

lr.predict(X_test)

test_labels[77]

test_texts[77]

import pickle
import os 
dest = os.path.join('Sentiment_Analysis', 'pkl_objects')
if not os.path.exists(dest):
  os.makedirs(dest)
  
# pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol = 4)

pickle.dump(lr, open(os.path.join(dest, 'classifier.pkl'), 'wb'))
pickle.dump(cv1, open(os.path.join(dest, 'vectorizer.pkl'), "wb"))

